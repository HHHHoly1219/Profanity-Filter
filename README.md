# ­ЪџФ Profanity Filter for Chinese Social Media РђћРђћ Variant Detection with Word Embeddings

This project aims to address the problem of profanities and their creative variants on Chinese social media platforms.  
Traditional keyword filters can be easily bypassed, e.g., Рђютѓ╗жђ╝РђЮ (idiot) may appear as РђюуЁъугћРђЮ, Рђютѓ╗Т»ћРђЮ, Рђютѓ╗bРђЮ, etc.  
This project leverages **word embeddings (word2vec)** to recognize and filter such variants, improving accuracy and robustness.  

---

## ­ЪЊќ Background
On platforms like Bilibili and Douyin, user comments often contain offensive language.  
Existing methods include:
- Keyword filtering
- Sentiment analysis
- Human moderation
- Machine Learning & NLP

However, the **variant phenomenon** in Chinese (homophones, character substitutions) makes detection more challenging compared to English .

---

## ­Ъј» Objective
- Detect Chinese profanity and its disguised variants  
- Automatically screen comments containing inappropriate language  
- Provide a cleaner and healthier online environment  

---

## ­ЪЏа№ИЈ Methodology
1. **Corpus**: Tencent AI Lab Chinese word embeddings (100/200 dimensions, millions of words)  
2. **Target Words**: Selected typical profanities (Рђютѓ╗жђ╝РђЮ, РђюТЊЇРђЮ)  
3. **Model Loading**: Using `gensim` to load embeddings  
4. **Variant Detection**: `most_similar()` function to find semantically close variants  

---
## ­ЪЊі Results
	Рђб	Successfully identified multiple variants of Рђютѓ╗жђ╝РђЮ and РђюТЊЇРђЮ
	Рђб	Demonstrated the effectiveness of word embeddings for detecting disguised Chinese profanity

---

## ­Ъџђ Future Work
	Рђб	Integrate larger corpora
	Рђб	Expand to more categories of sensitive words
	Рђб	Combine with sentiment analysis and deep learning models for improved accuracy

---

## ­ЪЎї Acknowledgments

Special thanks to Prof. Elaine U├Г Dhonnchad for assistance in corpus searching.
Corpus reference: [Tencent AI Lab Embedding](https://metatext.io/datasets/tencent-ai-lab-embedding-corpus)

---

Example results:  
```python
>>> w2v_model.most_similar("тѓ╗жђ╝", topn=5)
[('уЁъугћ', 0.9753), ('тѓ╗Т»ћ', 0.9524), ('тѓ╗жђ╝тЋі', 0.9431), ('тѓ╗b', 0.9292), ('тцДтѓ╗жђ╝', 0.9223)]

>>> w2v_model.most_similar("ТЊЇ", topn=5)
[('тдѕуџё', 0.7752), ('уІЌТЌЦуџё', 0.7749), ('С╗ќтеўуџё', 0.7738), ('С╗ќтдѕуџё', 0.7730), ('ТѕЉТЊЇ', 0.7590)]
